{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Protein Atlas Image Classification\n",
    "\n",
    "to Jan 11, 2019\n",
    "\n",
    "class number : 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas     as pd\n",
    "import numpy      as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmyk_array_unify(ary_c, ary_m, ary_y, ary_k):\n",
    "    \n",
    "    # 一次元化して配列が同じか。\n",
    "    len_c = len(ary_c.reshape(-1,))\n",
    "    len_m = len(ary_m.reshape(-1,))\n",
    "    len_y = len(ary_y.reshape(-1,))\n",
    "    len_k = len(ary_k.reshape(-1,))\n",
    "\n",
    "    \n",
    "    if( len_c - len_m + len_y - len_k ) == 0 :\n",
    "       cmyk = []\n",
    "       \n",
    "       d_y_2 = len(ary_c)\n",
    "       d_x_2 = len(ary_c[0])\n",
    "       \n",
    "       for i in range(d_y_2):\n",
    "           d_x_3 = []\n",
    "           for j in range(d_x_2):\n",
    "               d_z_3 = []\n",
    "               d_z_3.append(ary_c[i][j])\n",
    "               d_z_3.append(ary_m[i][j])\n",
    "               d_z_3.append(ary_y[i][j])\n",
    "               d_z_3.append(ary_k[i][j])\n",
    "               d_x_3.append(d_z_3)\n",
    "           cmyk.append(d_x_3)\n",
    "       return np.array(cmyk)\n",
    "       \n",
    "    else:\n",
    "       print(\"配列の長さが違う。\")\n",
    "       return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpaic_image_loader(img_name, folder=\"./data/train/\", resize=(128,128), dim_1 = True):\n",
    "    \"\"\"\n",
    "    image loading and unifying as 3-level tensor\n",
    "    \"\"\"\n",
    "    img_blue   = cv2.resize(cv2.imread( folder + img_name + \"_blue\"   + \".png\", 0), dsize = resize)\n",
    "    img_green  = cv2.resize(cv2.imread( folder + img_name + \"_green\"  + \".png\", 0), dsize = resize)\n",
    "    img_red    = cv2.resize(cv2.imread( folder + img_name + \"_red\"    + \".png\", 0), dsize = resize)\n",
    "    img_yellow = cv2.resize(cv2.imread( folder + img_name + \"_yellow\" + \".png\", 0), dsize = resize)\n",
    "    \n",
    "    if dim_1:\n",
    "        return cmyk_array_unify(img_blue, img_green, img_red, img_yellow).reshape(-1,)\n",
    "        \n",
    "    return cmyk_array_unify(img_blue, img_green, img_red, img_yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def o_train_test_split(x_array, y_array, ratio=0.7):\n",
    "    \"\"\"\n",
    "    rand\n",
    "    \"\"\"\n",
    "    x_train_len         = int( len(x_array) * ratio )\n",
    "    x_train_index_array = random.sample(range(0, len(x_array), 1), k = x_train_len)\n",
    "    x_test_index_array  = list(set(range(0, len(x_array), 1)) - set(x_train_index_array))\n",
    "    x_train_array       = [x_array[i] for i in x_train_index_array]\n",
    "    x_test_array        = [x_array[i] for i in x_test_index_array]\n",
    "    y_train_array       = [y_array[i] for i in x_train_index_array]\n",
    "    y_test_array        = [y_array[i] for i in x_test_index_array]\n",
    "    \n",
    "    return (x_train_array, x_test_array, y_train_array, y_test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    " \n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2,2, 1],\n",
    "                        strides=[1,2,2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_4x4(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 4,4, 1],\n",
    "                        strides=[1,4,4, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_val(y_step_2, t_step_2):\n",
    "    length = len(y_step_2)\n",
    "    f_array = []\n",
    "    for index in range(length):\n",
    "        TP = sum([1 if (a_step == 1 and b_step == 1) else 0 for a_step, b_step in zip(y_step_2[index], t_step_2[index])])\n",
    "        FP = sum([1 if (a_step == 1 and b_step == 0) else 0 for a_step, b_step in zip(y_step_2[index], t_step_2[index])])\n",
    "        FN = sum([1 if (a_step == 0 and b_step == 1) else 0 for a_step, b_step in zip(y_step_2[index], t_step_2[index])])\n",
    "        f_array.append( TP / ( TP + ( FP + FN ) / 2 ))\n",
    "    return f_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "### Data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yの読み出し（最初の9個）\n",
    "train_df = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id\n",
    "image_name = np.array(train_df[\"Id\"][:])\n",
    "\n",
    "# target_vector\n",
    "t_data = np.array(train_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name_array, X_test_name_array, y_train_array, y_test_array = o_train_test_split(image_name, t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size       = 500\n",
    "x_train_name_len = len(X_train_name_array)\n",
    "batch_count      = int(x_train_name_len / batch_size) # 21750 / 50 = 435\n",
    "batch_count_mod  = x_train_name_len % batch_size      # 21750 % 50 = 72\n",
    "\n",
    "# randomにindexの値がとられる。[29800, 23100, 26200, 4800, 28800, 13800, 27100, 11200,]\n",
    "b_top_index_array = random.sample(range(0, x_train_name_len, batch_size), k = batch_count-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 5500, 7500, 9000, 19000, 19500, 16000, 2000, 15500, 1000, 2500, 9500, 11000, 20000, 12000, 3000, 14500, 8500, 15000, 8000, 4000, 12500, 5000, 18000, 1500, 0, 4500, 10000, 17000, 16500, 6000, 11500, 21500, 20500, 3500, 17500, 13000, 10500, 6500, 13500, 21000, 18500]\n",
      "21500\n"
     ]
    }
   ],
   "source": [
    "print(b_top_index_array)\n",
    "print((batch_count) * batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 他クラス分類問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_y = np.array([[0.2, 0.9, 0.2, 0.9, 0],[0.2, 0.9, 0.2, 0.9, 0],[0.2, 0.9, 0.2, 0.9, 0.8]])\n",
    "# sample_t = np.array([[0.1, 0.2, 0.8, 0.9, 0.6],[0.1, 0.2, 0.8, 0.9, 0.6],[0.5, 0.2, 0.8, 0.9, 0.3]])\n",
    "\n",
    "\n",
    "# 出てくる配列 y = 0 ~ -1 [[0,0,10.8,],[],[],[],[]]\n",
    "\n",
    "# F値を求めて、tf.reduce_mean する。この段階で、a = [0~1]×入力値数（バッチ数）\n",
    "#accuracy = tf.reduce_mean(tf.cast(a, \"float\"))\n",
    "\n",
    "\n",
    "# まずは誤差関数がちゃんと動くか確認\n",
    "# tensor →rリストとりだし\n",
    "# リスト計算→tensor\n",
    "# tensorflow でf値を求めるやつがあるか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### マルチクラス分類問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# 入力層\n",
    "x = tf.placeholder(\"float\", [None, 65536])\n",
    "\n",
    "# 形状変更\n",
    "x_image = tf.reshape(x, [-1, 128, 128, 4])\n",
    "\n",
    "# 第1層 (バッチ正規化層)\n",
    "y_norm1 = tf.contrib.layers.batch_norm(x_image)\n",
    "\n",
    "# 第2層 (畳み込み層)\n",
    "W_conv1 = weight_variable([3, 3, 4, 8])\n",
    "b_conv1 = bias_variable([8])\n",
    "y_conv1 = tf.nn.relu(conv2d(y_norm1, W_conv1) + b_conv1)\n",
    "\n",
    "# 第3層 (バッチ正規化層)\n",
    "y_norm2 = tf.contrib.layers.batch_norm(y_conv1)\n",
    "\n",
    "# 第4層 (畳み込み層)\n",
    "W_conv2 = weight_variable([3, 3, 8, 8])\n",
    "b_conv2 = bias_variable([8])\n",
    "y_conv2 = tf.nn.relu(conv2d(y_norm2, W_conv2) + b_conv2)\n",
    "\n",
    "# 第5層 (バッチ正規化層)\n",
    "y_norm3 = tf.contrib.layers.batch_norm(y_conv2)\n",
    "\n",
    "# 第6層 (畳み込み層)\n",
    "W_conv3 = weight_variable([3, 3, 8, 16])\n",
    "b_conv3 = bias_variable([16])\n",
    "y_conv3 = tf.nn.relu(conv2d(y_norm3, W_conv3) + b_conv3)\n",
    "\n",
    "# 第7層 (バッチ正規化層)\n",
    "y_norm4 = tf.contrib.layers.batch_norm(y_conv3)\n",
    "\n",
    "# 第8層 (プーリング層)\n",
    "y_pool1 = max_pool_2x2(y_norm4) #64*64\n",
    "y_drop1 = tf.nn.dropout(y_pool1, keep_prob)\n",
    "\n",
    "# 第9層 (畳み込み層)\n",
    "W_conv4 = weight_variable([3, 3, 16, 32])\n",
    "b_conv4 = bias_variable([32])\n",
    "y_conv4 = tf.nn.relu(conv2d(y_drop1, W_conv4) + b_conv4)\n",
    "\n",
    "# 第10層 (バッチ正規化層)\n",
    "y_norm5 = tf.contrib.layers.batch_norm(y_conv4)\n",
    "\n",
    "# 第11層 (プーリング層)\n",
    "y_pool2 = max_pool_2x2(y_norm5) #32*32\n",
    "y_drop2 = tf.nn.dropout(y_pool2, keep_prob)\n",
    "\n",
    "# 第12層 (畳み込み層)\n",
    "W_conv5 = weight_variable([3, 3, 32, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "y_conv5 = tf.nn.relu(conv2d(y_drop2, W_conv5) + b_conv5)\n",
    "\n",
    "# 第13層 (バッチ正規化層)\n",
    "y_norm6 = tf.contrib.layers.batch_norm(y_conv5)\n",
    "\n",
    "# 第14層 (プーリング層)\n",
    "y_pool3 = max_pool_2x2(y_norm6) #16*16\n",
    "y_drop3 = tf.nn.dropout(y_pool3, keep_prob)\n",
    "\n",
    "# 第15層 (畳み込み層)\n",
    "W_conv6 = weight_variable([3, 3, 64, 128])\n",
    "b_conv6 = bias_variable([128])\n",
    "y_conv6 = tf.nn.relu(conv2d(y_drop3, W_conv6) + b_conv6)\n",
    "\n",
    "# 第16層 (バッチ正規化層)\n",
    "y_norm7 = tf.contrib.layers.batch_norm(y_conv6)\n",
    "\n",
    "\n",
    "# 第17層 (プーリング層)\n",
    "y_pool4 = max_pool_2x2(y_norm7) #8*8\n",
    "y_drop4 = tf.nn.dropout(y_pool4, keep_prob)\n",
    "\n",
    "# 第18層 (畳み込み層)\n",
    "W_conv7 = weight_variable([3, 3, 128, 256])\n",
    "b_conv7 = bias_variable([256])\n",
    "y_conv7 = tf.nn.relu(conv2d(y_drop4, W_conv7) + b_conv7)\n",
    "\n",
    "# 第19層 (バッチ正規化層)\n",
    "y_norm8 = tf.contrib.layers.batch_norm(y_conv7)\n",
    "\n",
    "# 第20層 (プーリング層)\n",
    "y_pool5 = max_pool_2x2(y_norm8) #4*4\n",
    "y_drop5 = tf.nn.dropout(y_pool5, keep_prob)\n",
    "\n",
    "# 第21層 (平坦化層)\n",
    "y_pool2_flat = tf.reshape(y_drop5, [-1, 4096])\n",
    " \n",
    "# 第22層 (全結合層)\n",
    "W_fc1 = weight_variable([4096, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "y_fc1 = tf.nn.relu(tf.matmul(y_pool2_flat, W_fc1) + b_fc1)\n",
    " \n",
    "# 第23層 (全結合層)\n",
    "W_fc2 = weight_variable([1024, 28])\n",
    "b_fc2 = bias_variable([28])\n",
    "y = tf.sigmoid(tf.matmul(y_fc1, W_fc2) + b_fc2) #<= シグモイド関数\n",
    " \n",
    "# 目標出力の次元\n",
    "t = tf.placeholder(\"float\", [None, 28])\n",
    "\n",
    "# 損失関数を計算グラフを作成する\n",
    "# cross_entropy = -tf.reduce_sum(t * tf.log(y))\n",
    "# cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=t,logits=y)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum( t * tf.log(y + 1e-9)) + ((1-t) * tf.log(1 - y + 1e-9)  )\n",
    "\n",
    "# 次の(1)、(2)を行うための計算グラフを作成する。\n",
    "# (1) 損失関数に対するネットワークを構成するすべての変数の勾配を計算する。\n",
    "# (2) 勾配方向に学習率分移動して、すべての変数を更新する。\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    " \n",
    "# 初期化を行うための計算グラフを作成する。\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    " \n",
    "# テストデータに対する正答率を計算するための計算グラフを作成する。\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "# f1_score, update_op = tf.contrib.metrics.f1_score(t, y)\n",
    "\n",
    "\n",
    "# y_f = tf.cast(y, \"float\")\n",
    "# t_f = tf.cast(t, \"float\")\n",
    "# tp = tf.math.reduce_sum(tf.count_nonzero(y_f * t_f), 0)\n",
    "# tn = tf.math.reduce_sum(tf.count_nonzero((y_f - 1.) * (t_f - 1.)), 0)\n",
    "# fp = tf.math.reduce_sum(tf.count_nonzero(y_f * (t_f - 1.)), 0)\n",
    "# fn = tf.math.reduce_sum(tf.count_nonzero((y_f - 1.) * t_f), 0)\n",
    "\n",
    "\n",
    "# # acc       = (tp + tn) / (tp + fp + fn + tn)\n",
    "# # precision = tp / (tp + fp)\n",
    "# # recall    = tp / (tp + fn)\n",
    "# # fmeasure  = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# p = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "# r = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "# f1 = 2*p*r / (p+r+tf.keras.backend.epsilon())\n",
    "# f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\n",
    "# f_score  = 1 - tf.reduce_mean(f1)\n",
    "\n",
    "# # tf_f1 = tf_f1_score(t, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                          | 0/42 [00:00<?, ?it/s]\n",
      "  2%|█▌                                                                | 1/42 [00:25<17:27, 25.55s/it]\n",
      "  5%|███▏                                                              | 2/42 [00:47<16:21, 24.54s/it]\n",
      "  7%|████▋                                                             | 3/42 [01:10<15:30, 23.86s/it]\n",
      " 10%|██████▎                                                           | 4/42 [01:32<14:55, 23.56s/it]\n",
      " 12%|███████▊                                                          | 5/42 [01:55<14:21, 23.29s/it]\n",
      " 14%|█████████▍                                                        | 6/42 [02:17<13:46, 22.95s/it]\n",
      " 17%|███████████                                                       | 7/42 [02:40<13:18, 22.82s/it]\n",
      " 19%|████████████▌                                                     | 8/42 [03:02<12:52, 22.72s/it]\n",
      " 21%|██████████████▏                                                   | 9/42 [03:25<12:30, 22.76s/it]\n",
      " 24%|███████████████▍                                                 | 10/42 [03:48<12:08, 22.77s/it]\n",
      " 26%|█████████████████                                                | 11/42 [04:11<11:45, 22.75s/it]\n",
      " 29%|██████████████████▌                                              | 12/42 [04:33<11:19, 22.64s/it]\n",
      " 31%|████████████████████                                             | 13/42 [04:56<11:02, 22.85s/it]\n",
      " 33%|█████████████████████▋                                           | 14/42 [05:19<10:42, 22.95s/it]\n",
      " 36%|███████████████████████▏                                         | 15/42 [05:43<10:23, 23.09s/it]\n",
      " 38%|████████████████████████▊                                        | 16/42 [06:07<10:07, 23.36s/it]\n",
      " 40%|██████████████████████████▎                                      | 17/42 [06:31<09:49, 23.56s/it]\n",
      " 43%|███████████████████████████▊                                     | 18/42 [06:55<09:27, 23.65s/it]\n",
      " 45%|█████████████████████████████▍                                   | 19/42 [07:18<09:00, 23.52s/it]\n",
      " 48%|██████████████████████████████▉                                  | 20/42 [07:41<08:36, 23.46s/it]\n",
      " 50%|████████████████████████████████▌                                | 21/42 [08:05<08:13, 23.49s/it]\n",
      " 52%|██████████████████████████████████                               | 22/42 [08:28<07:50, 23.52s/it]\n",
      " 55%|███████████████████████████████████▌                             | 23/42 [08:52<07:26, 23.48s/it]\n",
      " 57%|█████████████████████████████████████▏                           | 24/42 [09:15<07:02, 23.47s/it]\n",
      " 60%|██████████████████████████████████████▋                          | 25/42 [09:39<06:40, 23.57s/it]\n",
      " 62%|████████████████████████████████████████▏                        | 26/42 [10:03<06:20, 23.79s/it]\n",
      " 64%|█████████████████████████████████████████▊                       | 27/42 [10:28<05:58, 23.90s/it]\n",
      " 67%|███████████████████████████████████████████▎                     | 28/42 [10:51<05:34, 23.91s/it]\n",
      " 69%|████████████████████████████████████████████▉                    | 29/42 [11:16<05:12, 24.04s/it]\n",
      " 71%|██████████████████████████████████████████████▍                  | 30/42 [11:40<04:47, 23.98s/it]\n",
      " 74%|███████████████████████████████████████████████▉                 | 31/42 [12:04<04:23, 23.99s/it]\n",
      " 76%|█████████████████████████████████████████████████▌               | 32/42 [12:27<03:58, 23.88s/it]\n",
      " 79%|███████████████████████████████████████████████████              | 33/42 [12:51<03:34, 23.82s/it]\n",
      " 81%|████████████████████████████████████████████████████▌            | 34/42 [13:15<03:10, 23.84s/it]\n",
      " 83%|██████████████████████████████████████████████████████▏          | 35/42 [13:39<02:47, 23.92s/it]\n",
      " 86%|███████████████████████████████████████████████████████▋         | 36/42 [14:03<02:23, 23.95s/it]\n",
      " 88%|█████████████████████████████████████████████████████████▎       | 37/42 [14:27<02:00, 24.01s/it]\n",
      " 90%|██████████████████████████████████████████████████████████▊      | 38/42 [14:51<01:35, 24.00s/it]\n",
      " 93%|████████████████████████████████████████████████████████████▎    | 39/42 [15:04<01:02, 20.74s/it]\n",
      " 95%|█████████████████████████████████████████████████████████████▉   | 40/42 [15:28<00:43, 21.65s/it]\n",
      " 98%|███████████████████████████████████████████████████████████████▍ | 41/42 [15:51<00:22, 22.20s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████| 42/42 [16:15<00:00, 22.63s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████| 1/1 [16:15<00:00, 975.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# セッションを作成して、計算グラフを実行する。\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "    # 初期化を実行する。\n",
    "    sess.run(init_g)\n",
    "    sess.run(init_l)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # 学習を実行する。（エポック）\n",
    "    for i in tqdm(range(1)):\n",
    "        train_f   = 0\n",
    "        #test_acc  = 0\n",
    "        # ミニパッチ\n",
    "        j = 0\n",
    "        for p_index in tqdm(b_top_index_array):\n",
    "            j = j + 1\n",
    "            # 最後の配列だった場合\n",
    "            if p_index == batch_count * batch_size:\n",
    "                index_array = list(range(p_index, p_index + batch_count_mod )) #　-１いらない？(とった)\n",
    "            else:\n",
    "                index_array = list(range(p_index, p_index + batch_size))\n",
    "            \n",
    "            # ここで読み出し。\n",
    "            patch_x = [hpaic_image_loader(X_train_name_array[i]) for i in index_array]\n",
    "            patch_t = [y_train_array[i] for i in index_array]\n",
    "            \n",
    "            sess.run(train_step, feed_dict={x: patch_x, t: patch_t, keep_prob: 0.5})\n",
    "            \n",
    "            # trainデータの訓練精度を追加、表示\n",
    "#             train_acc += sess.run(accuracy, feed_dict = {x: patch_x, t: patch_t, keep_prob: 1.0})\n",
    "            # train_f   += sess.run(f_score,  feed_dict = {x: patch_x, t: patch_t, keep_prob: 1.0})\n",
    "            # print('tr_f:{} \\n '  .format(train_f   / j ))\n",
    "            \n",
    "\n",
    "\n",
    "             \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"C:/Users/talla/Dropbox/Computer/Repositry/Self/human-protein-atlas/model_1/hpaic_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Restore and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/talla/Dropbox/Computer/Repositry/Self/human-protein-atlas/model_1/hpaic_model\n",
      "Restored a model\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init_g)\n",
    "sess.run(init_l)\n",
    "saver = tf.train.import_meta_graph('./model_1/hpaic_model.meta')\n",
    "saver.restore(sess,  tf.train.latest_checkpoint('./model_1/'))\n",
    "print('Restored a model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yの読み出し（最初の9個）\n",
    "sub_df = pd.read_csv(\"./data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 20.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.39it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 24.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 23.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 20.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 23.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 24.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 22.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:22<00:00, 21.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 702/702 [00:31<00:00, 22.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 名前の読み出し。\n",
    "sub_image_name  = np.array(sub_df[\"Id\"][:])\n",
    "\n",
    "size        = 500\n",
    "length      = len(sub_image_name)\n",
    "count       = int(length / size) # 21750 / 500 = 43\n",
    "count_mod   = length % size      # 21750 % 500 = 250\n",
    "\n",
    "pred_test = []\n",
    "for i in range(count):\n",
    "    s_index = i*size\n",
    "    e_index = i*size+ size\n",
    "    \n",
    "    if i == count -1:\n",
    "        e_index = e_index + count_mod\n",
    "    \n",
    "    sub_image_array = np.array([hpaic_image_loader(j, \"./data/test/\" ) for j in tqdm(sub_image_name[s_index : e_index])])\n",
    "    pred_test.extend(sess.run(y, feed_dict={x: sub_image_array, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1 if j>=0.5 else 0 for  j in i]for i in pred_test]\n",
    "index_array = [[ str(j) if i[j] else \" \"  for j in range(len(i)) ]for i in a]\n",
    "string_array = [[ j for j in i if j != \" \"]for i in index_array]\n",
    "join_array = [' '.join(i) for i in string_array]\n",
    "submit = pd.DataFrame({\"Id\":sub_image_name, \"Predicted\":join_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"submit/submit.csv\" , columns=['Id',\"Predicted\" ], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11702\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflowのgpu動作確認●\n",
    "# テストデータの評価\n",
    "# 答えの出力（方法、形式、確認）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataとtrain_dataで訓練\n",
    "X_train, X_test,y_train, y_test = train_test_split(x_data, t_data, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 31072/31072 [4:32:11<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# yをもとにxを読み出し。(_blue, _green, _red, _yellow)でテンソル３階層する。\n",
    "cmyk = [\"_blue\",\"_green\",\"_red\",\"_yellow\"]\n",
    "x_list = []\n",
    "\n",
    "for img in tqdm(image_name):\n",
    "    img_blue   = cv2.imread( \"./data/train/\" + img + \"_blue\"   + \".png\", 0)\n",
    "    img_green  = cv2.imread( \"./data/train/\" + img + \"_green\"  + \".png\", 0)\n",
    "    img_red    = cv2.imread( \"./data/train/\" + img + \"_red\"    + \".png\", 0)\n",
    "    img_yellow = cv2.imread( \"./data/train/\" + img + \"_yellow\" + \".png\", 0)\n",
    "    \n",
    "    cmyk       = cmyk_array_unify(img_blue, img_green, img_red, img_yellow)\n",
    "    cmyk_plane = cmyk.reshape(-1,)\n",
    "    x_list.append(cmyk_plane)\n",
    "    \n",
    "x_data = np.array(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Shape_17:0\", shape=(3,), dtype=int32)\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'DecodePng_17:0' shape=(?, ?, 3) dtype=uint8>>\n",
      "Tensor(\"ToFloat_4:0\", shape=(?, ?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_r = tf.read_file(\"/data/train/00ab10d6-bba4-11e8-b2b9-ac1f6b6435d0_blue.png\")\n",
    "image = tf.image.decode_png(image_r, channels=3)\n",
    "image_float = tf.to_float(image)\n",
    "print(tf.shape(image))\n",
    "print(image.get_shape)\n",
    "print(image_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41 45 34 ...  0  0  0]\n",
      " [43 50 44 ...  0  0  0]\n",
      " [42 46 51 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sessionの作成\n",
    "\n",
    "\"\"\"\n",
    "#　訓練データの読み込み\n",
    "np.set_printoptions(threshold=10)\n",
    "img = cv2.imread(\"./data/train/00ab10d6-bba4-11e8-b2b9-ac1f6b6435d0_blue.png\",0)\n",
    "print(img)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_c = np.array([[\"a\",\"b\",\"c\"],[\"d\",\"e\",\"f\"]])\n",
    "z_m = np.array([[\"g\",\"h\",\"i\"],[\"j\",\"k\",\"l\"]])\n",
    "z_y = np.array([[\"m\",\"n\",\"o\"],[\"p\",\"q\",\"r\"]])\n",
    "z_k = np.array([[\"s\",\"t\",\"u\"],[\"v\",\"w\",\"x\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['a' 'g' 'm' 's']\n",
      "  ['b' 'h' 'n' 't']\n",
      "  ['c' 'i' 'o' 'u']]\n",
      "\n",
      " [['d' 'j' 'p' 'v']\n",
      "  ['e' 'k' 'q' 'w']\n",
      "  ['f' 'l' 'r' 'x']]]\n"
     ]
    }
   ],
   "source": [
    "s = cmyk_array_unify(z_c,z_m,z_y,z_k)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'g' 'm' ... 'l' 'r' 'x']\n"
     ]
    }
   ],
   "source": [
    "x_image = s.reshape(-1,)\n",
    "print(x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#コードの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3786672106358165647, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6674410373\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 2611446402891614919\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu動作確認コード\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(262144,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dataの読み込み\n",
    "\n",
    "train_labels = pd.read_csv(\"data/train_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = tf.Variable(tf.zeros([2,1])) #入力は２次元\n",
    "#     b = tf.Variable(tf.zeros([1]))   #入力は２次元\n",
    "\n",
    "#     # def y(x):\n",
    "#     #     return sigmoid(np.dot(w,x)+b)\n",
    "#     # def sigmoid(x):\n",
    "#     #     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 512]) #入力\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1]) #正解出力\n",
    "y = tf.nn.sigmoid(tf.matmul(x,w)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-6aef72370c84>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\talla\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\talla\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\talla\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\talla\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\talla\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "x_batch, t_batch = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12156864 0.21960786 0.6\n",
      " 0.7686275  0.9960785  0.8196079  0.6039216  0.60784316 0.6039216\n",
      " 0.5529412  0.21960786 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.227451   0.69803923 0.77647066\n",
      " 0.56078434 0.9176471  0.9921569  0.9803922  0.9921569  0.95294124\n",
      " 0.95294124 0.9803922  0.9921569  0.9921569  0.9921569  0.9921569\n",
      " 0.36078432 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.35686275 0.9921569  0.9921569  0.9921569  0.9921569  0.79215693\n",
      " 0.34901962 0.21176472 0.34509805 0.         0.         0.23529413\n",
      " 0.34901962 0.34901962 0.34901962 0.34901962 0.15294118 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0509804  0.8117648  0.98823535\n",
      " 0.91372555 0.9450981  0.34901962 0.0627451  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.43137258 0.9921569  0.91372555 0.         0.1254902\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.43137258\n",
      " 0.9921569  0.91372555 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.43137258 0.9921569  0.91372555\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.43137258 0.9921569  0.9607844  0.28235295 0.15686275\n",
      " 0.04313726 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.28627452\n",
      " 0.9921569  0.9921569  0.9921569  0.9921569  0.82745105 0.54509807\n",
      " 0.23529413 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.37254903 0.8745099\n",
      " 0.9333334  0.9921569  0.9921569  0.9921569  0.98823535 0.74509805\n",
      " 0.5803922  0.07450981 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07843138 0.13333334 0.19215688\n",
      " 0.4784314  0.8000001  0.9803922  0.9921569  0.9921569  0.7803922\n",
      " 0.07450981 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14117648 0.6431373  0.9921569  0.9921569  0.80392164 0.07843138\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.03137255\n",
      " 0.3254902  0.9921569  0.9921569  0.3529412  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00784314 0.7490196\n",
      " 0.9921569  0.90196085 0.07843138 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.43529415 0.9921569  0.9921569\n",
      " 0.11764707 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14117648 0.7725491  0.9921569  0.9058824  0.07843138 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.05490196 0.44705886 0.9490197  0.9960785\n",
      " 0.9843138  0.3137255  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3254902\n",
      " 0.6431373  0.35686275 0.2509804  0.04313726 0.3529412  0.5686275\n",
      " 0.8352942  0.9921569  0.9921569  0.9921569  0.3529412  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3647059  0.9921569  0.9960785\n",
      " 0.9843138  0.96470594 0.9921569  0.9921569  0.9960785  0.9921569\n",
      " 0.6901961  0.18431373 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01176471 0.4666667  0.94117653 0.9921569  0.9921569\n",
      " 0.9921569  0.77647066 0.6        0.21960786 0.04313726 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_batch[0])\n",
    "print(t_batch[0])\n",
    "print(type(x_batch[0]))\n",
    "print(type(t_batch[0]))\n",
    "\n",
    "#print(t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-c6070b6cb362>:3: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    img = sess.run(image)\n",
    "    Image.fromarray(np.uint8(img)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = np.array(data.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2150555571929651621, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 6871947673\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3101695577045397688\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_gpuenv)",
   "language": "python",
   "name": "conda_tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
